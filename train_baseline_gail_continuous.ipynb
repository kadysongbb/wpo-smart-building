{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import sinergym\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from stable_baselines import *\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.distributions as dist\n",
    "import tensorflow.contrib.layers as layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator Network Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator:\n",
    "    def __init__(self, sess, ob_shape, ac_shape, hidden_size, lr, name):\n",
    "        self.sess = sess\n",
    "        self.ob_shape = ob_shape\n",
    "        self.ac_shape = ac_shape\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lr = lr\n",
    "        self.name = name\n",
    "\n",
    "        self.ob_ac = tf.placeholder(dtype=tf.float32, shape=[None, 21])\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            self._build_network()\n",
    "\n",
    "\n",
    "    def _build_network(self):\n",
    "        with tf.variable_scope('discriminator'):\n",
    "            d_h1 = layers.fully_connected(self.ob_ac, self.hidden_size, activation_fn=tf.tanh)\n",
    "            d_h2 = layers.fully_connected(d_h1, self.hidden_size, activation_fn=tf.tanh)\n",
    "            d_out = layers.fully_connected(d_h2, 1, activation_fn=None)\n",
    "\n",
    "        self.reward = - tf.squeeze(tf.log(tf.sigmoid(d_out)))\n",
    "        \n",
    "        expert_out, policy_out = tf.split(d_out, num_or_size_splits=2, axis=0)\n",
    "\n",
    "        self.loss = (tf.losses.sigmoid_cross_entropy(tf.ones_like(policy_out), policy_out)\n",
    "                     + tf.losses.sigmoid_cross_entropy(tf.zeros_like(expert_out), expert_out))\n",
    "        \n",
    "        with tf.name_scope('train_op'):\n",
    "            grads = tf.gradients(self.loss, self.params())\n",
    "            self.grads = list(zip(grads, self.params()))\n",
    "            self.train_op = tf.train.AdamOptimizer(self.lr).apply_gradients(self.grads)\n",
    "\n",
    "    def params(self):\n",
    "        return tf.global_variables(self.name).copy()\n",
    "\n",
    "    def get_reward(self, expert_ob_ac):\n",
    "        feed_dict = {self.ob_ac: expert_ob_ac}\n",
    "        return self.sess.run(self.reward, feed_dict=feed_dict)\n",
    "\n",
    "    def update(self, all_ob_ac):\n",
    "        feed_dict = {self.ob_ac: all_ob_ac}\n",
    "\n",
    "        self.sess.run(self.train_op, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.common.policies import MlpPolicy, CnnPolicy, FeedForwardPolicy\n",
    "from stable_baselines.bench import Monitor\n",
    "import os\n",
    "\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "    def __init__(self, rl_env, discriminator, lamb):\n",
    "        super(CustomEnv, self).__init__()\n",
    "        self.action_space = rl_env.action_space\n",
    "        self.observation_space = rl_env.observation_space\n",
    "        self.rl_env = rl_env\n",
    "        self.discriminator = discriminator\n",
    "        self.lamb = lamb\n",
    "    \n",
    "    def step(self, action):\n",
    "        observation, rl_reward, done, info = self.rl_env.step(action)\n",
    "        il_reward = self.discriminator.get_reward((np.concatenate([observation, action], 0)).reshape(1,21))\n",
    "        reward = self.lamb*rl_reward + (1-self.lamb)*il_reward\n",
    "        return observation, reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        return self.rl_env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL + IL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines import TRPO, PPO2\n",
    "from stable_baselines.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines.ddpg.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "env = gym.make('Eplus-5Zone-mixed-continuous-v1')\n",
    "config = tf.ConfigProto(\n",
    "device_count={'GPU': 1},\n",
    "intra_op_parallelism_threads=1,\n",
    "allow_soft_placement=True\n",
    ")\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "tf.keras.backend.set_session(sess)\n",
    "discriminator = Discriminator(sess, env.observation_space.shape, env.action_space.shape, 10, 0.01, 'D')\n",
    "sess.run(tf.global_variables_initializer())\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Create log dir\n",
    "log_dir = \"tmp/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# the noise objects for DDPG\n",
    "n_actions = env.action_space.shape[-1]\n",
    "param_noise = None\n",
    "action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=float(0.1) * np.ones(n_actions))\n",
    "\n",
    "\n",
    "# Create and wrap the environment\n",
    "custom_env = CustomEnv(env, discriminator, 0.3)\n",
    "custom_env = Monitor(custom_env, log_dir, allow_early_resets=True)\n",
    "custom_env = DummyVecEnv(([lambda: custom_env]))\n",
    "data = np.load('expert_traj/ddpg_expert_5zonemix.npz')\n",
    "expert_obs = data['obs']\n",
    "expert_actions = data['actions']\n",
    "expert_ob_ac = np.concatenate([expert_obs, expert_actions], axis = 1)\n",
    "\n",
    "# Generate expert trajectories (train expert)\n",
    "model1 = PPO2('MlpPolicy', custom_env, verbose=0)\n",
    "\n",
    "ts = 0\n",
    "batch_ts = len(expert_ob_ac)\n",
    "total_ts = batch_ts * 50\n",
    "\n",
    "\n",
    "rew_sum = []\n",
    "while ts < total_ts:\n",
    "    model1.set_env(custom_env)\n",
    "    model1.learn(batch_ts)\n",
    "    model1.save(\"agent_simu\")\n",
    "    ts += batch_ts\n",
    "    \n",
    "    ### Option 2: RL after IL\n",
    "    eps = int(ts/len(expert_ob_ac))\n",
    "    lamb = 0.7\n",
    "    if eps <= 5:\n",
    "        lamb = 0.3\n",
    "    \n",
    "    obs = custom_env.reset()[0]\n",
    "    observes, actions, rewards = [],[],[]\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(expert_ob_ac):\n",
    "        observes.append(obs)\n",
    "        action, _states = model1.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        if done:\n",
    "            obs = custom_env.reset()[0]\n",
    "            rew_sum.append(np.sum(rewards))\n",
    "            rewards = []\n",
    "        i += 1\n",
    "        \n",
    "    policy_ob_ac = np.concatenate([observes, actions],1)\n",
    "    for k in range(5):\n",
    "        discriminator.update(np.concatenate([expert_ob_ac, policy_ob_ac], axis=0))\n",
    "    custom_env = CustomEnv(env, discriminator, lamb)\n",
    "    custom_env = DummyVecEnv(([lambda: custom_env]))\n",
    "    model1.load(\"agent_simu\")\n",
    "    print('Timesteps: ' + str(ts) + ' Rewards Sum: ' + str(rew_sum[-1]))\n",
    "print(rew_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IL only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines import PPO2\n",
    "from stable_baselines.common.vec_env import DummyVecEnv, VecNormalize\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "env = gym.make('Eplus-5Zone-mixed-continuous-v1')\n",
    "config = tf.ConfigProto(\n",
    "device_count={'GPU': 1},\n",
    "intra_op_parallelism_threads=1,\n",
    "allow_soft_placement=True\n",
    ")\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "tf.keras.backend.set_session(sess)\n",
    "discriminator = Discriminator(sess, env.observation_space.shape, env.action_space.shape, 10, 0.01, 'D')\n",
    "sess.run(tf.global_variables_initializer())\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Create log dir\n",
    "log_dir = \"tmp/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "        \n",
    "# Create and wrap the environmentc\n",
    "custom_env = CustomEnv(env, discriminator, 0.2)\n",
    "custom_env = Monitor(custom_env, log_dir, allow_early_resets=True)\n",
    "custom_env = DummyVecEnv(([lambda: custom_env]))\n",
    "data = np.load('expert_traj/ddpg_expert_5zonemix.npz')\n",
    "expert_obs = data['obs']\n",
    "expert_actions = data['actions']\n",
    "expert_ob_ac = np.concatenate([expert_obs, expert_actions], axis = 1)\n",
    "\n",
    "model2 = PPO2(MlpPolicy, custom_env, verbose=0)\n",
    "\n",
    "ts = 0\n",
    "batch_ts = len(expert_ob_ac)\n",
    "total_ts = batch_ts * 50\n",
    "lamb = 0.2\n",
    "\n",
    "rew_sum = []\n",
    "### IL only training\n",
    "while ts < total_ts:\n",
    "    model2.set_env(custom_env)\n",
    "    model2.learn(batch_ts)\n",
    "    model2.save(\"agent_il_only\")\n",
    "    ts += batch_ts\n",
    "    \n",
    "    obs = custom_env.reset()[0]\n",
    "    observes, actions, rewards = [],[],[]\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(expert_ob_ac):\n",
    "        observes.append(obs)\n",
    "        action, _states = model2.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        if done:\n",
    "            obs = custom_env.reset()[0]\n",
    "            rew_sum.append(np.sum(rewards))\n",
    "            rewards = []\n",
    "        i += 1\n",
    "        \n",
    "    policy_ob_ac = np.concatenate([observes, actions],1)\n",
    "    for k in range(5):\n",
    "        discriminator.update(np.concatenate([expert_ob_ac, policy_ob_ac], axis=0))\n",
    "    custom_env = CustomEnv(env, discriminator, lamb)\n",
    "    custom_env = DummyVecEnv(([lambda: custom_env]))\n",
    "    model2.load(\"agent_il_only\")\n",
    "    print('Timesteps: ' + str(ts) + ' Rewards Sum: ' + str(rew_sum[-1]))\n",
    "print(rew_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines import PPO2\n",
    "from stable_baselines.common.vec_env import DummyVecEnv, VecNormalize\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "env = gym.make('Eplus-5Zone-mixed-continuous-v1')\n",
    "config = tf.ConfigProto(\n",
    "device_count={'GPU': 1},\n",
    "intra_op_parallelism_threads=1,\n",
    "allow_soft_placement=True\n",
    ")\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "tf.keras.backend.set_session(sess)\n",
    "discriminator = Discriminator(sess, env.observation_space.shape, env.action_space.shape, 10, 0.01, 'D')\n",
    "sess.run(tf.global_variables_initializer())\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Create log dir\n",
    "log_dir = \"tmp/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "        \n",
    "# Create and wrap the environmentc\n",
    "custom_env = CustomEnv(env, discriminator, 1)\n",
    "custom_env = Monitor(custom_env, log_dir, allow_early_resets=True)\n",
    "custom_env = DummyVecEnv(([lambda: custom_env]))\n",
    "data = np.load('expert_traj/ddpg_expert_5zonemix.npz')\n",
    "expert_obs = data['obs']\n",
    "expert_actions = data['actions']\n",
    "expert_ob_ac = np.concatenate([expert_obs, expert_actions], axis = 1)\n",
    "\n",
    "model3 = PPO2(MlpPolicy, custom_env, verbose=0)\n",
    "\n",
    "ts = 0\n",
    "batch_ts = len(expert_ob_ac)\n",
    "total_ts = batch_ts * 50\n",
    "lamb = 1\n",
    "\n",
    "rew_sum = []\n",
    "### RL only training\n",
    "while ts < total_ts:\n",
    "    model3.set_env(custom_env)\n",
    "    model3.learn(batch_ts)\n",
    "    model3.save(\"agent_rl_only\")\n",
    "    ts += batch_ts\n",
    "    \n",
    "    obs = custom_env.reset()[0]\n",
    "    observes, actions, rewards = [],[],[]\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(expert_ob_ac):\n",
    "        observes.append(obs)\n",
    "        action, _states = model3.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        if done:\n",
    "            obs = custom_env.reset()[0]\n",
    "            rew_sum.append(np.sum(rewards))\n",
    "            rewards = []\n",
    "        i += 1\n",
    "        \n",
    "    policy_ob_ac = np.concatenate([observes, actions],1)\n",
    "    for k in range(5):\n",
    "        discriminator.update(np.concatenate([expert_ob_ac, policy_ob_ac], axis=0))\n",
    "    custom_env = CustomEnv(env, discriminator, lamb)\n",
    "    custom_env = DummyVecEnv(([lambda: custom_env]))\n",
    "    model3.load(\"agent_rl_only\")\n",
    "    print('Timesteps: ' + str(ts) + ' Rewards Sum: ' + str(rew_sum[-1]))\n",
    "print(rew_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRPO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines import TRPO\n",
    "from stable_baselines.common.vec_env import DummyVecEnv, VecNormalize\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "env = gym.make('Eplus-5Zone-mixed-continuous-v1')\n",
    "config = tf.ConfigProto(\n",
    "device_count={'GPU': 1},\n",
    "intra_op_parallelism_threads=1,\n",
    "allow_soft_placement=True\n",
    ")\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "tf.keras.backend.set_session(sess)\n",
    "discriminator = Discriminator(sess, env.observation_space.shape, env.action_space.shape, 10, 0.01, 'D')\n",
    "sess.run(tf.global_variables_initializer())\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Create log dir\n",
    "log_dir = \"tmp/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "        \n",
    "# Create and wrap the environmentc\n",
    "custom_env = CustomEnv(env, discriminator, 1)\n",
    "custom_env = Monitor(custom_env, log_dir, allow_early_resets=True)\n",
    "custom_env = DummyVecEnv(([lambda: custom_env]))\n",
    "data = np.load('expert_traj/ddpg_expert_5zonemix.npz')\n",
    "expert_obs = data['obs']\n",
    "expert_actions = data['actions']\n",
    "expert_ob_ac = np.concatenate([expert_obs, expert_actions], axis = 1)\n",
    "\n",
    "model4 = TRPO(MlpPolicy, custom_env, verbose=0)\n",
    "\n",
    "ts = 0\n",
    "batch_ts = len(expert_ob_ac)\n",
    "total_ts = batch_ts * 50\n",
    "lamb = 1\n",
    "\n",
    "rew_sum = []\n",
    "### RL only training\n",
    "while ts < total_ts:\n",
    "    model4.set_env(custom_env)\n",
    "    model4.learn(batch_ts)\n",
    "    model4.save(\"agent_rl_trpo_only\")\n",
    "    ts += batch_ts\n",
    "    \n",
    "    obs = custom_env.reset()[0]\n",
    "    observes, actions, rewards = [],[],[]\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(expert_ob_ac):\n",
    "        observes.append(obs)\n",
    "        action, _states = model4.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        if done:\n",
    "            obs = custom_env.reset()[0]\n",
    "            rew_sum.append(np.sum(rewards))\n",
    "            rewards = []\n",
    "        i += 1\n",
    "        \n",
    "    policy_ob_ac = np.concatenate([observes, actions],1)\n",
    "    for k in range(5):\n",
    "        discriminator.update(np.concatenate([expert_ob_ac, policy_ob_ac], axis=0))\n",
    "    custom_env = CustomEnv(env, discriminator, lamb)\n",
    "    custom_env = DummyVecEnv(([lambda: custom_env]))\n",
    "    model4.load(\"agent_rl_trpo_only\")\n",
    "    print('Timesteps: ' + str(ts) + ' Rewards Sum: ' + str(rew_sum[-1]))\n",
    "print(rew_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2C "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines import A2C\n",
    "from stable_baselines.common.vec_env import DummyVecEnv, VecNormalize\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "env = gym.make('Eplus-5Zone-mixed-continuous-v1')\n",
    "config = tf.ConfigProto(\n",
    "device_count={'GPU': 1},\n",
    "intra_op_parallelism_threads=1,\n",
    "allow_soft_placement=True\n",
    ")\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "tf.keras.backend.set_session(sess)\n",
    "discriminator = Discriminator(sess, env.observation_space.shape, env.action_space.shape, 10, 0.01, 'D')\n",
    "sess.run(tf.global_variables_initializer())\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Create log dir\n",
    "log_dir = \"tmp/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "        \n",
    "# Create and wrap the environmentc\n",
    "custom_env = CustomEnv(env, discriminator, 1)\n",
    "custom_env = Monitor(custom_env, log_dir, allow_early_resets=True)\n",
    "custom_env = DummyVecEnv(([lambda: custom_env]))\n",
    "data = np.load('expert_traj/ddpg_expert_5zonemix.npz')\n",
    "expert_obs = data['obs']\n",
    "expert_actions = data['actions']\n",
    "expert_ob_ac = np.concatenate([expert_obs, expert_actions], axis = 1)\n",
    "\n",
    "model5 = A2C(MlpPolicy, custom_env, verbose=0)\n",
    "\n",
    "ts = 0\n",
    "batch_ts = len(expert_ob_ac)\n",
    "total_ts = batch_ts * 50\n",
    "lamb = 1\n",
    "\n",
    "rew_sum = []\n",
    "### RL only training\n",
    "while ts < total_ts:\n",
    "    model5.set_env(custom_env)\n",
    "    model5.learn(batch_ts)\n",
    "    model5.save(\"agent_rl_a2c_only\")\n",
    "    ts += batch_ts\n",
    "    \n",
    "    obs = custom_env.reset()[0]\n",
    "    observes, actions, rewards = [],[],[]\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(expert_ob_ac):\n",
    "        observes.append(obs)\n",
    "        action, _states = model5.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        if done:\n",
    "            obs = custom_env.reset()[0]\n",
    "            rew_sum.append(np.sum(rewards))\n",
    "            rewards = []\n",
    "        i += 1\n",
    "        \n",
    "    policy_ob_ac = np.concatenate([observes, actions],1)\n",
    "    for k in range(5):\n",
    "        discriminator.update(np.concatenate([expert_ob_ac, policy_ob_ac], axis=0))\n",
    "    custom_env = CustomEnv(env, discriminator, lamb)\n",
    "    custom_env = DummyVecEnv(([lambda: custom_env]))\n",
    "    model5.load(\"agent_rl_a2c_only\")\n",
    "    print('Timesteps: ' + str(ts) + ' Rewards Sum: ' + str(rew_sum[-1]))\n",
    "print(rew_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDPG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines import DDPG\n",
    "from stable_baselines.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines.ddpg.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "env = gym.make('Eplus-5Zone-mixed-continuous-v1')\n",
    "config = tf.ConfigProto(\n",
    "device_count={'GPU': 1},\n",
    "intra_op_parallelism_threads=1,\n",
    "allow_soft_placement=True\n",
    ")\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "tf.keras.backend.set_session(sess)\n",
    "discriminator = Discriminator(sess, env.observation_space.shape, env.action_space.shape, 10, 0.01, 'D')\n",
    "sess.run(tf.global_variables_initializer())\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Create log dir\n",
    "log_dir = \"tmp/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "        \n",
    "# Create and wrap the environmentc\n",
    "custom_env = CustomEnv(env, discriminator, 1)\n",
    "custom_env = Monitor(custom_env, log_dir, allow_early_resets=True)\n",
    "custom_env = DummyVecEnv(([lambda: custom_env]))\n",
    "data = np.load('expert_traj/ddpg_expert_5zonemix.npz')\n",
    "expert_obs = data['obs']\n",
    "expert_actions = data['actions']\n",
    "expert_ob_ac = np.concatenate([expert_obs, expert_actions], axis = 1)\n",
    "\n",
    "# the noise objects for DDPG\n",
    "n_actions = env.action_space.shape[-1]\n",
    "param_noise = None\n",
    "action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=float(0.1) * np.ones(n_actions))\n",
    "\n",
    "# Generate expert trajectories (train expert)\n",
    "model6 = DDPG('MlpPolicy', custom_env, param_noise=param_noise, action_noise=action_noise, verbose=0)\n",
    "\n",
    "ts = 0\n",
    "batch_ts = len(expert_ob_ac)\n",
    "total_ts = batch_ts * 50\n",
    "lamb = 1\n",
    "\n",
    "rew_sum = []\n",
    "### RL only training\n",
    "while ts < total_ts:\n",
    "    model6.set_env(custom_env)\n",
    "    model6.learn(batch_ts)\n",
    "    model6.save(\"agent_rl_ddpg_only\")\n",
    "    ts += batch_ts\n",
    "    \n",
    "    obs = custom_env.reset()[0]\n",
    "    observes, actions, rewards = [],[],[]\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(expert_ob_ac):\n",
    "        observes.append(obs)\n",
    "        action, _states = model2.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        if done:\n",
    "            obs = custom_env.reset()[0]\n",
    "            rew_sum.append(np.sum(rewards))\n",
    "            rewards = []\n",
    "        i += 1\n",
    "        \n",
    "    policy_ob_ac = np.concatenate([observes, actions],1)\n",
    "    for k in range(5):\n",
    "        discriminator.update(np.concatenate([expert_ob_ac, policy_ob_ac], axis=0))\n",
    "    custom_env = CustomEnv(env, discriminator, lamb)\n",
    "    custom_env = DummyVecEnv(([lambda: custom_env]))\n",
    "    model6.load(\"agent_rl_ddpg_only\")\n",
    "    print('Timesteps: ' + str(ts) + ' Rewards Sum: ' + str(rew_sum[-1]))\n",
    "print(rew_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule-based "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines import TRPO\n",
    "from stable_baselines.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from gym import spaces\n",
    "\n",
    "# Create log dir\n",
    "log_dir = \"tmp/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "        \n",
    "# Create and wrap the environmentc\n",
    "env = gym.make('Eplus-5Zone-mixed-continuous-v1')\n",
    "data = np.load('expert_traj/ddpg_expert_5zonemix.npz')\n",
    "expert_obs = data['obs']\n",
    "expert_actions = data['actions']\n",
    "expert_ob_ac = np.concatenate([expert_obs, expert_actions], axis = 1)\n",
    "\n",
    "config = tf.ConfigProto(\n",
    "device_count={'GPU': 1},\n",
    "intra_op_parallelism_threads=1,\n",
    "allow_soft_placement=True\n",
    ")\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "tf.keras.backend.set_session(sess)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "obs = env.reset()[0]\n",
    "observes, actions, rewards = [],[],[]\n",
    "i = 0\n",
    "while i < len(expert_ob_ac):\n",
    "    observes.append(obs)\n",
    "    action = np.asarray([20, 26])\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    actions.append(action)\n",
    "    rewards.append(reward)\n",
    "    i += 1\n",
    "print(' Rewards Sum: ' + str(np.sum(rewards)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate expert trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import sinergym\n",
    "import numpy as np\n",
    "from stable_baselines import SAC, PPO2, TRPO, TD3, DDPG\n",
    "from stable_baselines.gail import generate_expert_traj\n",
    "from stable_baselines.ddpg.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "\n",
    "env = gym.make('Eplus-5Zone-mixed-continuous-v1')\n",
    "\n",
    "# the noise objects for DDPG\n",
    "n_actions = env.action_space.shape[-1]\n",
    "param_noise = None\n",
    "action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=float(0.1) * np.ones(n_actions))\n",
    "\n",
    "# Generate expert trajectories (train expert)\n",
    "model = DDPG('MlpPolicy', env, param_noise=param_noise, action_noise=action_noise, verbose=1)\n",
    "# Train for n_timesteps timesteps and record n_episodes trajectories\n",
    "# all the data will be saved in '*.npz' file\n",
    "generate_expert_traj(model, 'ddpg_expert_5zonemix', n_timesteps=1000000, n_episodes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.common.vec_env import DummyVecEnv, VecNormalize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create log dir\n",
    "log_dir = \"tmp/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "env = gym.make('Eplus-5Zone-mixed-continuous-v1')\n",
    "env = Monitor(env, log_dir, allow_early_resets=True)\n",
    "env = DummyVecEnv(([lambda: env]))\n",
    "obs = env.reset()\n",
    "i = 0\n",
    "violation = 0\n",
    "total_timesteps = 24*31*3\n",
    "temp_list = []\n",
    "total_power = 0\n",
    "while i < total_timesteps:\n",
    "    action, _states = model5.predict(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    temp = obs[0][8]\n",
    "    temp_list.append(temp)\n",
    "    total_power += info[0]['total_power']\n",
    "    if temp > 23.5001 or temp < 19.999:\n",
    "        violation += 1\n",
    "    i += 1\n",
    "print(violation/total_timesteps)\n",
    "print(total_power/(31*24))\n",
    "\n",
    "fig = plt.figure(figsize=(10,3))\n",
    "ax = fig.add_subplot(111)\n",
    "int_list = list(range(1,32))\n",
    "string_label_list = [str(x) for x in int_list]\n",
    "ax.set_xticks(list(range(0,24*31*3,24*3)))\n",
    "ax.set_xticklabels(string_label_list)\n",
    "ax.plot(np.arange(total_timesteps), temp_list)\n",
    "print(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
